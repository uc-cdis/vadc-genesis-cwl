apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: gwas-template-database-version
  annotations:
    version: 1.0.7
    description: |
        * Database version added to the gwas_metadata.yaml
spec:
  # must complete in 8h (28,800 seconds)
  activeDeadlineSeconds: 28800
  # keep workflow for 10 seconds
  ttlStrategy:
    secondsAfterCompletion: 10
  podMetadata:
    annotations:
      karpenter.sh/do-not-evict: "true"
  entrypoint: gwas-workflow
  templates:
    - name: gwas-workflow
      inputs:
        parameters:
          - name: internal_api_env
            default: default
          - name: source_id
          - name: source_population_cohort
          - name: hare_concept_id
            default: 2000007027
          - name: hare_population
          - name: n_pcs
            default: 0
          - name: variables
          - name: out_prefix
            value: vadc_genesis
          - name: outcome
          - name: pca_file
          - name: relatedness_matrix_file
          - name: genome_build
            enum:
              - "hg38"
              - "hg19"
            default: "hg19"
          - name: n_segments
            default: 0
          - name: segment_length
            default: 10000
          - name: variant_block_size
            default: 1024
          - name: mac_threshold
          - name: maf_threshold
            default: 0.01
          - name: imputation_score_cutoff
            default: 0.03
          - name: pvalue_cutoff
            default: 5e-8
          - name: top_n_hits
            default: 100
          - name: gds_files
          - name: sex_table
          - name: team_project
      dag:
        tasks:
          - name: get-downloadable-bucket
            template: get-downloadable-bucket
          - name: pass-variables
            template: pass-variables
            arguments:
              parameters:
                - name: variables
                  value: "{{inputs.parameters.variables}}"
          - name: process-variables
            template: process-variables
            dependencies: [pass-variables]
            arguments:
              parameters:
                - name: internal_api_env
                  value: "{{inputs.parameters.internal_api_env}}"
                - name: hare_concept_id
                  value: "{{inputs.parameters.hare_concept_id}}"
                - name: outcome
                  value: "{{inputs.parameters.outcome}}"
              artifacts:
                - name: variables_json
                  from: "{{tasks.pass-variables.outputs.artifacts.variables_json}}"
          - name: generate-attrition-csv
            template: generate-attrition-csv
            dependencies: [get-downloadable-bucket, process-variables]
            arguments:
              parameters:
                - name: internal_api_env
                  value: "{{inputs.parameters.internal_api_env}}"
                - name: source_id
                  value: "{{inputs.parameters.source_id}}"
                - name: hare_concept_id
                  value: "{{inputs.parameters.hare_concept_id}}"
                - name: source_population_cohort
                  value: "{{inputs.parameters.source_population_cohort}}"
                - name: outcome
                  value: "{{inputs.parameters.outcome}}"
                - name: downloadable_bucket
                  value: "{{tasks.get-downloadable-bucket.outputs.parameters.downloadable_bucket}}"
              artifacts:
                - name: variables_validated
                  from: "{{tasks.process-variables.outputs.artifacts.variables_validated}}"
          - name: generate-gwas-metadata
            template: generate-gwas-metadata
            dependencies: [process-variables]
            arguments:
              parameters:
                - name: source_id
                  value: "{{inputs.parameters.source_id}}"
                - name: source_population_cohort
                  value: "{{inputs.parameters.source_population_cohort}}"
                - name: outcome
                  value: "{{inputs.parameters.outcome}}"
                - name: internal_api_env
                  value: "{{inputs.parameters.internal_api_env}}"
                - name: n_pcs
                  value: "{{inputs.parameters.n_pcs}}"
                - name: maf_threshold
                  value: "{{inputs.parameters.maf_threshold}}"
                - name: imputation_score_cutoff
                  value: "{{inputs.parameters.imputation_score_cutoff}}"
                - name: hare_population
                  value: "{{inputs.parameters.hare_population}}"
                - name: pvalue_cutoff
                  value: "{{inputs.parameters.pvalue_cutoff}}"
                - name: top_n_hits
                  value: "{{inputs.parameters.top_n_hits}}"
              artifacts:
                - name: variables_validated
                  from: "{{tasks.process-variables.outputs.artifacts.variables_validated}}"
          - name: get-pheno-csv
            template: get-pheno-csv
            dependencies: [process-variables]
            arguments:
              parameters:
                - name: source_id
                  value: "{{inputs.parameters.source_id}}"
                - name: internal_api_env
                  value: "{{inputs.parameters.internal_api_env}}"
                - name: source_population_cohort
                  value: "{{inputs.parameters.source_population_cohort}}"
              artifacts:
                - name: variables_with_hare_json
                  from: "{{tasks.process-variables.outputs.artifacts.variables_with_hare_json}}"
          - name: generate-pheno
            template: generate-pheno
            dependencies: [generate-attrition-csv, generate-gwas-metadata, get-pheno-csv]
            arguments:
              parameters:
                - name: prefixed_hare_concept_id
                  value: "ID_{{inputs.parameters.hare_concept_id}}"
                - name: hare_population 
                  value: "{{inputs.parameters.hare_population}}"
              artifacts:
                - name: pheno_csv
                  from: "{{tasks.get-pheno-csv.outputs.artifacts.pheno_csv}}"
          - name: run-null-model
            template: run-null-model
            dependencies: [generate-pheno]
            arguments:
              parameters:
                - name: n_pcs
                  value: "{{inputs.parameters.n_pcs}}"
                - name: pca_file
                  value: "{{inputs.parameters.pca_file}}"
                - name: relatedness_matrix_file
                  value: "{{inputs.parameters.relatedness_matrix_file}}"
                - name: out_prefix
                  value: "{{inputs.parameters.out_prefix}}_null_model"
                - name: covariates
                  value: "{{=jsonpath(tasks['process-variables'].outputs.parameters.other_variable_data, '$.covariates')}}"
                - name: outcome
                  value: "{{=jsonpath(tasks['process-variables'].outputs.parameters.other_variable_data, '$.outcome')}}"
                - name: outcome_type
                  value: "{{=jsonpath(tasks['process-variables'].outputs.parameters.other_variable_data, '$.outcome_type')}}"
              artifacts: 
                - name: phenotype_file
                  from: "{{tasks.generate-pheno.outputs.artifacts.phenotype_file}}"
          - name: combine-sex-table
            template: add-sex-info
            dependencies: [run-null-model]
            arguments:
              parameters:
                - name: sex_table
                  value: "{{inputs.parameters.sex_table}}"
              artifacts:
                - name: phenotype_from_null_model
                  from: "{{tasks.run-null-model.outputs.artifacts.null_model_phenotype}}"
          - name: run-impute-score-filter
            template: run-impute-score-filter
            dependencies: [generate-pheno]
            arguments:
              parameters:
                - name: impute_cutoff
                  value: "{{inputs.parameters.imputation_score_cutoff}}"
                - name: gds_file
                  value: "{{item}}"
            withParam: "{{=toJson(map(jsonpath(inputs.parameters.gds_files, '$'), {#}))}}"
          - name: define-segments
            template: define-segments
            arguments:
              parameters:
                - name: genome_build
                  value: "{{inputs.parameters.genome_build}}"
                - name: n_segments
                  value: "{{inputs.parameters.n_segments}}"
                - name: segment_length
                  value: "{{inputs.parameters.segment_length}}"
          - name: split-filename
            template: split-filename
            arguments:
              parameters:
                - name: gds_file
                  value: "{{=map(jsonpath(inputs.parameters.gds_files, '$'), {#})[0]}}"
          - name: filter-segments
            template: filter-segments
            dependencies: [define-segments, split-filename]
            arguments:
              parameters:
                - name: file_prefix
                  value: "{{=jsonpath(tasks['split-filename'].outputs.result, '$.file_prefix')}}"
                - name: file_suffix
                  value: "{{=jsonpath(tasks['split-filename'].outputs.result, '$.file_suffix')}}"
                - name: gds_filenames
                  value: "{{inputs.parameters.gds_files}}"
              artifacts:
                - name: segment_file
                  from: "{{tasks.define-segments.outputs.artifacts.segment_file}}"
          - name: run-single-assoc
            template: run-single-assoc
            dependencies: [run-null-model, combine-sex-table, filter-segments, split-filename, run-impute-score-filter]
            arguments:
              parameters:
                - name: file_prefix
                  value: "{{=jsonpath(tasks['split-filename'].outputs.result, '$.file_prefix')}}"
                - name: file_suffix
                  value: "{{=jsonpath(tasks['split-filename'].outputs.result, '$.file_suffix')}}"
                - name: gds_files
                  value: "{{inputs.parameters.gds_files}}"
                - name: genome_build
                  value: "{{inputs.parameters.genome_build}}"
                - name: out_prefix
                  value: "{{inputs.parameters.out_prefix}}"
                - name: variant_block_size
                  value: "{{inputs.parameters.variant_block_size}}"
                - name: outcome_type
                  value: "{{=jsonpath(tasks['process-variables'].outputs.parameters.other_variable_data, '$.outcome_type')}}"
                - name: mac_threshold
                  value: "{{inputs.parameters.mac_threshold}}"
                - name: maf_threshold
                  value: "{{inputs.parameters.maf_threshold}}"
                - name: segment
                  value: "{{item}}"
              artifacts:
                - name: segment_file
                  from: "{{tasks.define-segments.outputs.artifacts.segment_file}}"
                - name: phenotype_file
                  from: "{{tasks.combine-sex-table.outputs.artifacts.sex_merged_phenotype_file}}"
                - name: null_model_results
                  from: "{{tasks.run-null-model.outputs.artifacts.null_model_directory}}"
                - name: variant_list_dir
                  s3:
                    key: "{{workflow.name}}/variant_lists"
            withParam: "{{=toJson(jsonpath(tasks['filter-segments'].outputs.result, '$.segments'))}}"
          - name: combine-shards
            template: combine-shards
            dependencies: [run-single-assoc]
            arguments:
              parameters:
                - name: chromosome
                  value: "{{item}}"
                - name: out_prefix
                  value: "{{inputs.parameters.out_prefix}}"
              artifacts:
                - name: file_shards
                  s3:
                    key: "{{workflow.name}}/single_assoc_chunks"
            withParam: "{{=toJson(jsonpath(tasks['filter-segments'].outputs.result, '$.chromosomes'))}}"
          - name: run-plots
            template: run-plots
            dependencies: [get-downloadable-bucket, combine-shards]
            arguments:
              parameters:
                - name: chromosomes
                  value: "{{=sprig.join(' ', jsonpath(tasks['filter-segments'].outputs.result, '$.chromosomes'))}}"
                - name: out_prefix
                  value: "{{inputs.parameters.out_prefix}}"
                - name: downloadable_bucket
                  value: "{{tasks.get-downloadable-bucket.outputs.parameters.downloadable_bucket}}"
              artifacts:
                - name: combined
                  s3:
                    key: "{{workflow.name}}/single_assoc_combined"
          - name: annotate-statistics
            template: annotate-statistics
            dependencies: [combine-shards]
            arguments:
              parameters:
                - name: chromosome
                  value: "{{item}}"
                - name: out_prefix
                  value: "{{inputs.parameters.out_prefix}}"
              artifacts:
                - name: raw_stats
                  s3:
                    key: "{{workflow.name}}/single_assoc_combined/{{inputs.parameters.out_prefix}}_chr{{item}}.RData"
            withParam: "{{=toJson(jsonpath(tasks['filter-segments'].outputs.result, '$.chromosomes'))}}"
          - name: combine-pheweb
            template: combine-pheweb
            dependencies: [annotate-statistics]
            arguments:
              parameters:
                - name: out_prefix
                  value: "{{inputs.parameters.out_prefix}}"
              artifacts:
                - name: per_chrom_objs
                  s3:
                    key: "{{workflow.name}}/pheweb_objs"
          - name: generate-pheweb-json
            template: generate-pheweb-json
            dependencies: [get-downloadable-bucket, combine-pheweb]
            arguments:
              parameters:
                - name: out_prefix
                  value: "{{inputs.parameters.out_prefix}}"
                - name: downloadable_bucket
                  value: "{{tasks.get-downloadable-bucket.outputs.parameters.downloadable_bucket}}"
              artifacts:
                - name: pheweb_stats_tsv
                  from: "{{tasks.combine-pheweb.outputs.artifacts.pheweb_stats_tsv}}"
          - name: curate-gwas-hits
            template: curate-gwas-hits
            dependencies: [annotate-statistics]
            arguments:
              parameters:
                - name: pvalue_cutoff
                  value: "{{inputs.parameters.pvalue_cutoff}}"
                - name: top_n_hits
                  value: "{{inputs.parameters.top_n_hits}}"
                - name: out_prefix
                  value: "{{inputs.parameters.out_prefix}}"
              artifacts:
                - name: annotated_stats_dir
                  s3:
                    key: "{{workflow.name}}/annotated_csvs"
          - name: archive-outputs
            template: archive-outputs
            dependencies: [get-downloadable-bucket, run-plots, annotate-statistics, generate-gwas-metadata, generate-attrition-csv, curate-gwas-hits]
            arguments:
              parameters:
                - name: out_prefix
                  value: "{{inputs.parameters.out_prefix}}"
                - name: downloadable_bucket
                  value: "{{tasks.get-downloadable-bucket.outputs.parameters.downloadable_bucket}}"
              artifacts:
                - name: manhattan_plot
                  from: "{{tasks.run-plots.outputs.artifacts.manhattan_plot}}"
                - name: qq_plot 
                  from: "{{tasks.run-plots.outputs.artifacts.qq_plot}}"
                - name: annotated_csvs
                  s3:
                    key: "{{workflow.name}}/annotated_csvs"
                - name: gwas_metadata
                  from: "{{tasks.generate-gwas-metadata.outputs.artifacts.gwas_metadata}}"
                - name: attrition_csvs
                  from: "{{tasks.generate-attrition-csv.outputs.artifacts.attrition_table_directory}}"
                - name: top_hits_csv
                  from: "{{tasks.curate-gwas-hits.outputs.artifacts.top_hits_csv}}"
                - name: below_cutoff_csv
                  from: "{{tasks.curate-gwas-hits.outputs.artifacts.below_cutoff_csv}}"
          - name: create-indexd-record
            template: create-indexd-record
            dependencies: [get-downloadable-bucket, archive-outputs, generate-pheweb-json]
            arguments:
              parameters:
                - name: arborist_resource
                  value: "{{inputs.parameters.team_project}}"
                - name: internal_api_env
                  value: "{{inputs.parameters.internal_api_env}}"
                - name: out_prefix
                  value: "{{inputs.parameters.out_prefix}}"
                - name: downloadable_bucket
                  value: "{{tasks.get-downloadable-bucket.outputs.parameters.downloadable_bucket}}"
              artifacts:
                - name: gwas_archive
                  from: "{{tasks.archive-outputs.outputs.artifacts.gwas_archive}}"
                - name: manhattan_plot
                  from: "{{tasks.run-plots.outputs.artifacts.manhattan_plot}}"
                - name: attrition_json
                  from: "{{tasks.generate-attrition-csv.outputs.artifacts.attrition_table_json}}"
                - name: pheweb_manhattan_json
                  from: "{{tasks.generate-pheweb-json.outputs.artifacts.pheweb_manhattan_json}}"
                - name: pheweb_qq_json
                  from: "{{tasks.generate-pheweb-json.outputs.artifacts.pheweb_qq_json}}"

    - name: get-downloadable-bucket
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      script:
        image: quay.io/cdis/alpine-jq:latest
        imagePullPolicy: IfNotPresent
        command: [/bin/sh]
        source: |
          if [ -z $DOWNLOADABLE_BUCKET ]; then
            echo "Downloadable bucket is not found or set to empty string"
            exit 1
          else
            echo "Downloadable bucket found: $DOWNLOADABLE_BUCKET"
            echo "$DOWNLOADABLE_BUCKET" > /mnt/vol/downloadable_bucket.txt
          fi
        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
        env:
        - name: DOWNLOADABLE_BUCKET
          valueFrom:
            secretKeyRef:
              name: argo-template-values-secret
              key: DOWNLOADABLE_BUCKET
      outputs:
        parameters:
          - name: downloadable_bucket
            valueFrom:
              path: "/mnt/vol/downloadable_bucket.txt"

    - name: pass-variables
      inputs:
        parameters:
          - name: variables
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      script:
        image: quay.io/cdis/alpine-jq:latest
        imagePullPolicy: IfNotPresent
        command: [sh]
        source: |
            echo '{{inputs.parameters.variables}}'|jq -r . > /mnt/vol/variables.json
        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
      outputs:
        artifacts:
          - name: variables_json
            path: /mnt/vol/variables.json
            archive: 
              none: {}
            s3:
              key: "{{workflow.name}}/{{workflow.name}}.variables.json"
    
    - name: process-variables
      inputs:
        parameters:
          - name: internal_api_env
          - name: hare_concept_id
          - name: outcome
        artifacts:
          - name: variables_json
            path: /mnt/vol/variables.json
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      script:
        image: 707767160287.dkr.ecr.us-east-1.amazonaws.com/gen3/vadc-gwas-tools:feat_database_version
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash"]
        source: |
          /env/bin/vadc-gwas-tools ProcessInputVariables \
          --raw_variables_json {{inputs.artifacts.variables_json.path}} \
          --hare_concept_id {{inputs.parameters.hare_concept_id}} \
          --outcome '{{inputs.parameters.outcome}}' \
          --output_raw_variable_json /mnt/vol/variables_validated.json \
          --output_variable_json_w_hare /mnt/vol/variables_with_hare.json \
          --output_other_json /mnt/vol/other_variable_data.json
        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
        env:
          - name: GEN3_ENVIRONMENT
            value: "{{inputs.parameters.internal_api_env}}"
      outputs:
        parameters:
          - name: other_variable_data
            valueFrom:
              path: /mnt/vol/other_variable_data.json
        artifacts:
          - name: variables_validated
            path: /mnt/vol/variables_validated.json
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/{{workflow.name}}.variables_validated.json"
          - name: variables_with_hare_json
            path: /mnt/vol/variables_with_hare.json
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/{{workflow.name}}.variables_with_hare.json"
          - name: other_variable_data_json
            path: /mnt/vol/other_variable_data.json
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/{{workflow.name}}.other_variable_data.json"

    - name: generate-attrition-csv
      inputs: 
        parameters:
          - name: source_id
          - name: internal_api_env
          - name: hare_concept_id
          - name: source_population_cohort
          - name: outcome
          - name: downloadable_bucket
        artifacts:
          - name: variables_validated
            path: /mnt/vol/variables_validated.json
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      script:
        image: 707767160287.dkr.ecr.us-east-1.amazonaws.com/gen3/vadc-gwas-tools:feat_database_version
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash"]
        source: |
          mkdir /mnt/vol/attrition_csvs
          /env/bin/vadc-gwas-tools GetCohortAttritionTable \
          --source_id {{inputs.parameters.source_id}} \
          --source_population_cohort {{inputs.parameters.source_population_cohort}} \
          --outcome '{{inputs.parameters.outcome}}' \
          --variables_json {{inputs.artifacts.variables_validated.path}} \
          --prefixed_breakdown_concept_id {{inputs.parameters.hare_concept_id}} \
          --output_csv_prefix /mnt/vol/attrition_csvs/{{workflow.name}} \
          --output_combined_json /mnt/vol/{{workflow.name}}.attrition.json
        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
        env:
          - name: GEN3_ENVIRONMENT
            value: "{{inputs.parameters.internal_api_env}}"
      outputs:
        artifacts:
          - name: attrition_table_directory
            path: /mnt/vol/attrition_csvs
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/attrition_csvs"
          - name: attrition_table_json
            path: /mnt/vol/{{workflow.name}}.attrition.json
            archive:
              none: {}
            s3:
              endpoint: s3.amazonaws.com
              bucket: "{{inputs.parameters.downloadable_bucket}}"
              key: "{{workflow.name}}/{{workflow.name}}.attrition.json"
              # accessKeySecret:
              #   name: argo-s3-creds
              #   key: AccessKeyId
              # secretKeySecret:
              #   name: argo-s3-creds
              #   key: SecretAccessKey
    
    - name: generate-gwas-metadata
      inputs:
        parameters:
          - name: source_id
          - name: source_population_cohort
          - name: outcome
          - name: internal_api_env
          - name: n_pcs
          - name: maf_threshold
          - name: imputation_score_cutoff
          - name: hare_population
          - name: pvalue_cutoff
          - name: top_n_hits
        artifacts:
          - name: variables_validated
            path: /mnt/vol/variables_validated.json
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      script:
        image: 707767160287.dkr.ecr.us-east-1.amazonaws.com/gen3/vadc-gwas-tools:feat_database_version
        imagePullPolicy: IfNotPresent
        command: [/bin/bash]
        source: |
          /env/bin/vadc-gwas-tools GetGwasMetadata \
          --source_id {{inputs.parameters.source_id}} \
          --source_population_cohort {{inputs.parameters.source_population_cohort}} \
          --variables_json {{inputs.artifacts.variables_validated.path}} \
          --outcome '{{inputs.parameters.outcome}}' \
          --n_pcs {{inputs.parameters.n_pcs}} \
          --maf_threshold {{inputs.parameters.maf_threshold}} \
          --imputation_score_cutoff {{inputs.parameters.imputation_score_cutoff}} \
          --hare_population "{{inputs.parameters.hare_population}}" \
          --pvalue_cutoff {{inputs.parameters.pvalue_cutoff}} \
          --top_n_hits {{inputs.parameters.top_n_hits}} \
          --output /mnt/vol/{{workflow.name}}.gwas_metadata.yaml
        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
        env:
          - name: GEN3_ENVIRONMENT
            value: "{{inputs.parameters.internal_api_env}}"
      outputs:
        artifacts:
          - name: gwas_metadata
            path: /mnt/vol/{{workflow.name}}.gwas_metadata.yaml
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/{{pod.name}}/{{workflow.name}}.gwas_metadata.yaml"

    - name: get-pheno-csv
      inputs:
        parameters:
          - name: source_id
          - name: internal_api_env
          - name: source_population_cohort
        artifacts:
          - name: variables_with_hare_json
            path: /mnt/vol/variables_with_hare.json
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      script:
        image: 707767160287.dkr.ecr.us-east-1.amazonaws.com/gen3/vadc-gwas-tools:feat_database_version
        imagePullPolicy: IfNotPresent
        command: [/bin/bash]
        source: |
          /env/bin/vadc-gwas-tools GetCohortPheno \
          --source_id {{inputs.parameters.source_id}} \
          --source_population_cohort {{inputs.parameters.source_population_cohort}} \
          --variables_json {{inputs.artifacts.variables_with_hare_json.path}} \
          --output /mnt/vol/{{workflow.name}}.pheno.csv.gz
        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
        env: 
          - name: GEN3_ENVIRONMENT
            value: "{{inputs.parameters.internal_api_env}}"
      outputs:
        artifacts:
          - name: pheno_csv
            path: /mnt/vol/{{workflow.name}}.pheno.csv.gz
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/{{pod.name}}/{{workflow.name}}.pheno.csv.gz"

    - name: generate-pheno
      inputs:
        parameters:
          - name: prefixed_hare_concept_id
          - name: hare_population
        artifacts:
          - name: pheno_csv
            path: "/mnt/vol/phenotypes.csv.gz"
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      script:
        image: 707767160287.dkr.ecr.us-east-1.amazonaws.com/gen3/topmed-master:2.12.0
        imagePullPolicy: IfNotPresent
        securityContext:
          runAsUser: 0
        command: [Rscript]
        source: |
          library(Biobase)

          dat.csv <- read.csv(gzfile("{{inputs.artifacts.pheno_csv.path}}"),
                              stringsAsFactors=FALSE, na.strings=c("NA", ""))
          stopifnot("sample.id" %in% names(dat.csv))
          dat.csv$sample.id <- as.character(dat.csv$sample.id)

          # Filter to HARE population
          dat.csv <- subset(dat.csv, {{inputs.parameters.prefixed_hare_concept_id}} == "{{inputs.parameters.hare_population}}")

          # Remove NAs
          dat.csv <- na.omit(dat.csv)

          # Convert to AnnotatedDataFrame
          annot <- AnnotatedDataFrame(dat.csv)
          stopifnot("sample.id" %in% varLabels(annot))
          save(annot, file="/mnt/vol/phenotypes.Rdata")

        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 2Gi
      outputs:
        artifacts:
          - name: phenotype_file
            path: "/mnt/vol/phenotypes.Rdata"
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/{{pod.name}}/phenotypes.Rdata"

    - name: run-null-model
      inputs:
        parameters:
          - name: n_pcs
          - name: covariates
          - name: out_prefix
          - name: outcome
          - name: outcome_type
          - name: pca_file
          - name: relatedness_matrix_file
        artifacts:
          - name: phenotype_file
            path: "/mnt/vol/phenotypes.Rdata"
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      script:
        image: 707767160287.dkr.ecr.us-east-1.amazonaws.com/gen3/topmed-master:2.12.0
        imagePullPolicy: IfNotPresent
        securityContext:
          runAsUser: 0
        command: [bash]
        source: |
          export NSLOTS=2
          cd /mnt/vol

          CONFIG="null_model.config"

          echo "out_prefix {{inputs.parameters.out_prefix}}" > $CONFIG
          echo "phenotype_file {{inputs.artifacts.phenotype_file.path}}" >> $CONFIG
          echo "outcome {{inputs.parameters.outcome}}" >> $CONFIG
          if [[ "{{inputs.parameters.outcome_type}}" == "BINARY" ]]; then
              echo "family \"binomial\"" >> $CONFIG
          else
              echo "family \"gaussian\"" >> $CONFIG
          fi
          echo "n_pcs {{inputs.parameters.n_pcs}}" >> $CONFIG
          echo "pca_file {{inputs.parameters.pca_file}}" >> $CONFIG
          echo "relatedness_matrix_file {{inputs.parameters.relatedness_matrix_file}}" >> $CONFIG

          if [[ -z "{{inputs.parameters.covariates}}" ]]; then
              echo "No covariates provided."
          else
              echo "covars \"{{inputs.parameters.covariates}}\"" >> $CONFIG
          fi
          echo "out_phenotype_file {{inputs.parameters.out_prefix}}_phenotypes.Rdata" >> $CONFIG

          set -xe
          cat $CONFIG

          Rscript /usr/local/analysis_pipeline/R/null_model.R $CONFIG
          Rscript /usr/local/analysis_pipeline/R/null_model_report.R $CONFIG --version 2.12.0
          ls -al

          DATADIR={{inputs.parameters.out_prefix}}_datadir
          mkdir $DATADIR
          mv {{inputs.parameters.out_prefix}}*.RData $DATADIR/

          REPORTDIR={{inputs.parameters.out_prefix}}_reports
          mkdir $REPORTDIR
          mv *.html $REPORTDIR/
          mv *.Rmd $REPORTDIR/

        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
          - name: gateway
            mountPath: /commons-data
        resources:
          requests:
            cpu: 2500m
            memory: 2Gi
          limits:
            cpu: 5000m
            memory: 5Gi
      outputs:
        artifacts:
          - name: null_model_directory
            path: "/mnt/vol/{{inputs.parameters.out_prefix}}_datadir"
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/{{pod.name}}/{{inputs.parameters.out_prefix}}_datadir"
          - name: null_model_phenotype
            path: "/mnt/vol/{{inputs.parameters.out_prefix}}_phenotypes.Rdata"
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/{{pod.name}}/{{inputs.parameters.out_prefix}}_phenotypes.Rdata"
          - name: null_model_report_directory
            path: "/mnt/vol/{{inputs.parameters.out_prefix}}_reports"
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/{{pod.name}}/{{inputs.parameters.out_prefix}}_reports"

    - name: add-sex-info
      inputs:
        parameters:
          - name: sex_table
        artifacts:
          - name: phenotype_from_null_model
            path: "/mnt/vol/null_model_phenotype.RData"
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      script:
        image: 707767160287.dkr.ecr.us-east-1.amazonaws.com/gen3/topmed-master:2.12.0
        imagePullPolicy: IfNotPresent
        securityContext:
          runAsUser: 0
        command: [Rscript]
        source: |
          library(Biobase)

          # get workdit and set workdir
          getwd()
          setwd("/mnt/vol")

          # Load phenotype RData from run-null-model.default name is annot
          null_pheno <- get(load("{{inputs.artifacts.phenotype_from_null_model.path}}"))

          # Read sex table as data frame
          sex_df <- read.csv("{{inputs.parameters.sex_table}}", 
                              stringsAsFactors=FALSE, na.strings=c("NA", ""), 
                              header=TRUE)

          # Change the column name from PERSON_ID to sample.id 
          colnames(sex_df)[1] <- "sample.id"

          # create a new data frame merged using null_pheno and sex_df
          merged_df <- merge(pData(null_pheno), sex_df, by="sample.id", all.x=TRUE)

          # convert to AnnotatedDataFrame and save
          annot <- AnnotatedDataFrame(merged_df)
          stopifnot("sample.id"%in%varLabels(annot))
          save(annot, file="/mnt/vol/phenotypes_with_sex.RData")
        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
          - name: gateway
            mountPath: /commons-data
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 2Gi
      outputs:
        artifacts:
          - name: sex_merged_phenotype_file
            path: "/mnt/vol/phenotypes_with_sex.RData"
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/{{pod.name}}/phenotypes_with_sex.RData"

    - name: run-impute-score-filter
      inputs:
        parameters:
          - name: impute_cutoff
          - name: gds_file
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      script:
        image: 707767160287.dkr.ecr.us-east-1.amazonaws.com/gen3/topmed-master:2.12.0
        imagePullPolicy: IfNotPresent
        command: [Rscript]
        source: |
          library(SeqVarTools)

          gdsFile <- "{{inputs.parameters.gds_file}}"
          imputation.cutoff <- {{inputs.parameters.impute_cutoff}}
          chrom <- gsub("(.*)(chr[0-9X]+)(.*)", "\\2", basename(gdsFile))
          cat("gds:", gdsFile, "chromosome:", chrom, "\n")

          dir.create("/mnt/vol/variant_lists")
          outFile <- file.path("/mnt/vol/variant_lists", paste0(chrom, ".selected_variants.RData"))

          gds <- seqOpen(gdsFile)

          all.variant.ids <- seqGetData(gds, "variant.id")
          imputation.fit <- seqGetData(gds, "annotation/info/R2")
          sel <- imputation.fit >= imputation.cutoff | is.na(imputation.fit)
          variant.ids <- all.variant.ids[sel]
          save(variant.ids, file=outFile)
          seqClose(gds)
          
          cat("Total variants:", length(all.variant.ids),
              "\nDropped variants:", length(all.variant.ids) - length(variant.ids),
              "\nRemaining variants (%):", length(variant.ids),
                  paste0("(", round(100*(length(variant.ids)/length(all.variant.ids)), 2), "%)"), "\n")

        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
          - name: gateway
            mountPath: /commons-data
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 2Gi
      outputs:
        artifacts:
          - name: variant_list_dir
            path: "/mnt/vol/variant_lists"
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/variant_lists"

    - name: define-segments
      inputs:
        parameters:
          - name: genome_build
          - name: n_segments
          - name: segment_length
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      script:
        image: 707767160287.dkr.ecr.us-east-1.amazonaws.com/gen3/topmed-master:2.12.0
        imagePullPolicy: IfNotPresent
        command: [bash]
        source: |
          cd /mnt/vol
          echo "genome_build {{inputs.parameters.genome_build}}" > define_segments.config
          echo "out_file segments.txt" >> define_segments.config

          if [[ "{{inputs.parameters.n_segments}}" == "0" ]]; then
              NSEG=""
          else
              NSEG=" --n_segments {{inputs.parameters.n_segments}}"
          fi

          Rscript /usr/local/analysis_pipeline/R/define_segments.R \
          define_segments.config \
          --segment_length {{inputs.parameters.segment_length}}${NSEG}
        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
        resources:
          requests:
            cpu: 250m
            memory: 512Mi
          limits:
            cpu: 500m
            memory: 1Gi
      outputs:
        artifacts:
          - name: segment_file
            path: "/mnt/vol/segments.txt"
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/{{pod.name}}/segments.txt"

    - name: split-filename
      inputs:
        parameters:
          - name: gds_file
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      container:
        image: 707767160287.dkr.ecr.us-east-1.amazonaws.com/gen3/vadc-gwas-tools:feat_database_version
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash", "-c"]
        args: ["/env/bin/vadc-gwas-tools SplitFilenameByChr \
          --gds_file {{inputs.parameters.gds_file}}"]
        resources:
          requests:
            cpu: 50m
            memory: 50Mi
          limits:
            cpu: 100m
            memory: 100Mi

    - name: filter-segments
      inputs:
        parameters:
          - name: gds_filenames
          - name: file_prefix
          - name: file_suffix
        artifacts:
          - name: segment_file
            path: "/mnt/vol/segments.txt"
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      container:
        image: 707767160287.dkr.ecr.us-east-1.amazonaws.com/gen3/vadc-gwas-tools:feat_database_version
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash", "-c"]
        args: ["/env/bin/vadc-gwas-tools FilterSegments \
          --file_prefix {{inputs.parameters.file_prefix}} \
          --file_suffix {{inputs.parameters.file_suffix}} \
          --segment_file {{inputs.artifacts.segment_file.path}} \
          {{=sprig.join(' ', map(jsonpath(inputs.parameters.gds_filenames, '$'), {#}))}}"]
        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
        resources:
          requests:
            cpu: 50m
            memory: 50Mi
          limits:
            cpu: 100m
            memory: 100Mi

    - name: run-single-assoc
      inputs:
        parameters:
          - name: gds_files
          - name: out_prefix
          - name: genome_build
          - name: file_prefix
          - name: file_suffix
          - name: segment
          - name: variant_block_size
          - name: mac_threshold
          - name: maf_threshold
          - name: outcome_type
        artifacts:
          - name: segment_file
            path: "/mnt/vol/segments.txt"
          - name: null_model_results
            path: "/mnt/vol/null_model_results"
          - name: phenotype_file
            path: "/mnt/vol/phenotypes.RData"
          - name: variant_list_dir
            path: "/mnt/vol/variant_lists"
      retryStrategy:
        limit: "5"
        retryPolicy: "Always"
      script:
        image: 707767160287.dkr.ecr.us-east-1.amazonaws.com/gen3/topmed-master:2.12.0
        imagePullPolicy: IfNotPresent
        securityContext:
          runAsUser: 0
        command: [bash]
        source: |
          cd /mnt/vol
          for fil in {{=sprig.join(" ", map(jsonpath(inputs.parameters.gds_files, '$'), {#}))}}; do
              ln -s $fil .
          done

          set -xe
          CHROM="$(awk 'NR=={{=asInt(inputs.parameters.segment) + 1}} {print $1}' {{inputs.artifacts.segment_file.path}})" 
          echo $CHROM

          CONFIG="assoc_single.config"

          echo "out_prefix {{inputs.parameters.out_prefix}}" > $CONFIG
          echo "genome_build {{inputs.parameters.genome_build}}" >> $CONFIG
          echo "gds_file \"{{inputs.parameters.file_prefix}} {{inputs.parameters.file_suffix}}\"" >> $CONFIG
          echo "phenotype_file {{inputs.artifacts.phenotype_file.path}}" >> $CONFIG
          echo "segment_file {{inputs.artifacts.segment_file.path}}" >> $CONFIG
          echo "variant_block_size {{inputs.parameters.variant_block_size}}" >> $CONFIG
          echo "variant_include_file {{inputs.artifacts.variant_list_dir.path}}/chr${CHROM}.selected_variants.RData" >> $CONFIG
          if [[ "{{inputs.parameters.outcome_type}}" == "BINARY" ]]; then
                echo "null_model_file {{inputs.artifacts.null_model_results.path}}/{{inputs.parameters.out_prefix}}_null_model.RData" >> $CONFIG
                echo "test_type \"score.spa\"" >> $CONFIG
          else
                echo "null_model_file {{inputs.artifacts.null_model_results.path}}/{{inputs.parameters.out_prefix}}_null_model_invnorm.RData" >> $CONFIG
          fi

          if [[ "{{inputs.parameters.mac_threshold}}" == "0" ]]; then
              echo "mac_threshold NA" >> $CONFIG
              echo "maf_threshold {{inputs.parameters.maf_threshold}}" >> $CONFIG
          else
              echo "mac_threshold {{inputs.parameters.mac_threshold}}" >> $CONFIG
          fi

          cat $CONFIG
          ls -al

          Rscript /usr/local/analysis_pipeline/R/assoc_single.R $CONFIG --chromosome $CHROM \
          --segment {{inputs.parameters.segment}} \
          --num_cores 1

          SEGOUT="{{inputs.parameters.out_prefix}}_chr${CHROM}_seg{{inputs.parameters.segment}}.RData"
          mkdir single_assoc_chunks

          if [[ -f $SEGOUT ]]; then
                mv $SEGOUT single_assoc_chunks/
          else
                echo "No outputs for segment... skipping..."
          fi

        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
          - name: gateway
            mountPath: /commons-data
        resources:
          requests:
            cpu: 1000m
            memory: 6Gi
          limits:
            cpu: 2000m
            memory: 12Gi
      outputs:
        artifacts:
          - name: single_assoc_segment_output
            path: "/mnt/vol/single_assoc_chunks"
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/single_assoc_chunks"

    - name: combine-shards
      inputs:
        parameters:
          - name: chromosome
          - name: out_prefix
        artifacts:
          - name: file_shards
            path: "/mnt/vol/file_shards"
      retryStrategy:
        limit: "4"
        retryPolicy: "Always"
      script:
        image: 707767160287.dkr.ecr.us-east-1.amazonaws.com/gen3/topmed-master:2.12.0
        imagePullPolicy: IfNotPresent
        securityContext:
          runAsUser: 0
        command: [bash]
        source: |
          cd {{inputs.artifacts.file_shards.path}}

          echo "out_prefix {{inputs.parameters.out_prefix}}" > assoc_combine.config
          echo "assoc_type \"single\"" >> assoc_combine.config

          cat assoc_combine.config
          ls -al

          set -xe
          Rscript /usr/local/analysis_pipeline/R/assoc_combine.R \
          assoc_combine.config \
          --chromosome {{inputs.parameters.chromosome}}
          mkdir single_assoc_combined
          mv {{inputs.parameters.out_prefix}}_chr{{inputs.parameters.chromosome}}.RData single_assoc_combined/

        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
        resources:
          requests:
            cpu: 1000m
            memory: 4Gi
          limits:
            cpu: 2000m
            memory: 8Gi
      outputs:
        artifacts:
          - name: single_assoc_combined
            path: "{{inputs.artifacts.file_shards.path}}/single_assoc_combined"
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/single_assoc_combined"

    - name: run-plots
      inputs:
        parameters:
          - name: chromosomes
          - name: out_prefix
          - name: downloadable_bucket
        artifacts:
          - name: combined
            path: "/mnt/vol/combined"
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      script:
        image: 707767160287.dkr.ecr.us-east-1.amazonaws.com/gen3/topmed-master:2.12.0
        imagePullPolicy: IfNotPresent
        securityContext:
          runAsUser: 0
        command: [bash]
        source: |
          cd {{inputs.artifacts.combined.path}}

          echo "assoc_type \"single\"" > plots.config
          echo "thin \"TRUE\"" >> plots.config
          echo "assoc_file \"{{inputs.parameters.out_prefix}}_chr .RData\"" >> plots.config
          echo "chromosomes \"{{inputs.parameters.chromosomes}}\"" >> plots.config
          echo "signif_type fixed" >> plots.config
          echo "signif_line_fixed 5e-8" >> plots.config
          echo "out_file_manh \"{{inputs.parameters.out_prefix}}_manhattan.png\"" >> plots.config
          echo "out_file_qq \"{{inputs.parameters.out_prefix}}_qq.png\"" >> plots.config

          cat plots.config
          ls -al

          set -xe
          Rscript /commons-data/assoc_plots.R plots.config

        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
          - name: gateway
            mountPath: /commons-data
        resources:
          requests:
            cpu: 250m
            memory: 7Gi
          limits:
            cpu: 1000m
            memory: 10Gi
      outputs:
        artifacts:
          - name: manhattan_plot
            path: "{{inputs.artifacts.combined.path}}/{{inputs.parameters.out_prefix}}_manhattan.png"
            archive:
              none: {}
            s3:
              endpoint: s3.amazonaws.com
              bucket: "{{inputs.parameters.downloadable_bucket}}"
              key: "{{workflow.name}}/{{workflow.name}}.{{inputs.parameters.out_prefix}}_manhattan.png"
              # accessKeySecret:
              #   name: argo-s3-creds
              #   key: AccessKeyId
              # secretKeySecret:
              #   name: argo-s3-creds
              #   key: SecretAccessKey
          - name: qq_plot
            path: "{{inputs.artifacts.combined.path}}/{{inputs.parameters.out_prefix}}_qq.png"
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/gwas_plots/{{inputs.parameters.out_prefix}}_qq.png"

    - name: annotate-statistics
      inputs:
        parameters:
          - name: chromosome
          - name: out_prefix
        artifacts:
          - name: raw_stats
            path: "/mnt/vol/raw_stats.RData"
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      script:
        image: 707767160287.dkr.ecr.us-east-1.amazonaws.com/gen3/topmed-master:2.12.0
        imagePullPolicy: IfNotPresent
        securityContext:
          runAsUser: 0
        command: [Rscript]
        source: |
          library(dplyr)

          # Load summary stats
          load("{{inputs.artifacts.raw_stats.path}}")

          # Load RSID meta
          anno.df <- read.delim("/commons-data/kmhernan/marker_metadata/vadc_mvp_marker_annotation.chr{{inputs.parameters.chromosome}}.vep.tsv.gz",
                                header=TRUE,
                                stringsAsFactors=FALSE,
                                sep="\t")

          # Load var meta
          varmeta.df <- read.csv("/commons-data/kmhernan/marker_metadata/vadc_mvp_marker_annotation.chr{{inputs.parameters.chromosome}}.var_meta.csv.gz",
                                 stringsAsFactors=FALSE) %>%
            mutate(chr=as.character(chr))

          # Make annotated df
          annotated.summary.df <- varmeta.df %>%
            left_join(anno.df, by=c("variant.name"="variant.name")) %>%
            inner_join(assoc, by=c("variant.id"="variant.id", "chr"="chr", "pos"="pos")) %>%
            mutate(chr=paste0("chr", chr))

          # Write new CSV
          write.csv(annotated.summary.df,
            file=gzfile("/mnt/vol/{{inputs.parameters.out_prefix}}_chr{{inputs.parameters.chromosome}}.annotated_summary.csv.gz"),
            row.names=FALSE,
            na="")

          # Format for pheweb
          if("SPA.pval" %in% names(annotated.summary.df)){
            fmt.df <- annotated.summary.df %>%
              mutate(chrom=gsub("chr([0-9X]+)", "\\1", chr)) %>%
              select(chrom, pos, ref, alt, rsids, nearest_genes, SPA.pval, Est, Est.SE, freq) %>%
              rename(pval=SPA.pval, beta=Est, sebeta=Est.SE, af=freq)
          } else {
            fmt.df <- annotated.summary.df %>%
              mutate(chrom=gsub("chr([0-9X]+)", "\\1", chr)) %>%
              select(chrom, pos, ref, alt, rsids, nearest_genes, Score.pval, Est, Est.SE, freq) %>%
              rename(pval=Score.pval, beta=Est, sebeta=Est.SE, af=freq)
          }

          # write pheweb RData
          save(fmt.df,
            file="/mnt/vol/{{inputs.parameters.out_prefix}}.chr{{inputs.parameters.chromosome}}.annotated_summary.pheweb.RData")

        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
          - name: gateway
            mountPath: /commons-data
        resources:
          requests:
            cpu: 250m
            memory: 1Gi
          limits:
            cpu: 500m
            memory: 4Gi
      outputs:
        artifacts:
          - name: annotated_stats_csv
            path: "/mnt/vol/{{inputs.parameters.out_prefix}}_chr{{inputs.parameters.chromosome}}.annotated_summary.csv.gz"
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/annotated_csvs/{{inputs.parameters.out_prefix}}_chr{{inputs.parameters.chromosome}}.annotated_summary.csv.gz"
          - name: pheweb_stats_obj
            path: "/mnt/vol/{{inputs.parameters.out_prefix}}.chr{{inputs.parameters.chromosome}}.annotated_summary.pheweb.RData"
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/pheweb_objs/{{inputs.parameters.out_prefix}}.chr{{inputs.parameters.chromosome}}.annotated_summary.pheweb.RData"

    - name: combine-pheweb
      inputs:
        parameters:
          - name: out_prefix
        artifacts:
          - name: per_chrom_objs
            path: "/mnt/vol/combined"
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      script:
        image: 707767160287.dkr.ecr.us-east-1.amazonaws.com/gen3/topmed-master:2.12.0
        imagePullPolicy: IfNotPresent
        securityContext:
          runAsUser: 0
        command: [Rscript]
        source: |
          library(TopmedPipeline)

          # Order files
          files.all <- sapply(c(1:22, "X"), function(x){
            paste0("{{inputs.artifacts.per_chrom_objs.path}}/",
              "{{inputs.parameters.out_prefix}}.chr", x, ".annotated_summary.pheweb.RData")
          })
          files <- Filter(function(x) file.exists(x), files.all)

          # Combine
          comb.dat <- dplyr::bind_rows(lapply(unname(files), getobj))

          # write pheweb TSV
          write.table(comb.dat,
            file=gzfile("/mnt/vol/{{inputs.parameters.out_prefix}}.combined_annotated_summary.pheweb.tsv.gz"),
            quote=FALSE,
            sep="\t",
            row.names=FALSE,
            na="")

        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
        resources:
          requests:
            cpu: 250m
            memory: 4Gi
          limits:
            cpu: 500m
            memory: 10Gi
      outputs:
        artifacts:
          - name: pheweb_stats_tsv 
            path: "/mnt/vol/{{inputs.parameters.out_prefix}}.combined_annotated_summary.pheweb.tsv.gz"
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/{{pod.name}}/{{inputs.parameters.out_prefix}}.combined_annotated_summary.pheweb.tsv.gz"

    - name: generate-pheweb-json
      inputs:
        parameters:
          - name: out_prefix
          - name: downloadable_bucket
        artifacts:
          - name: pheweb_stats_tsv
            path: "/mnt/vol/pheweb_stats.tsv.gz"
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      script:
        image: 707767160287.dkr.ecr.us-east-1.amazonaws.com/gen3/vadc-gwas-tools:feat_database_version
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash"]
        source: |
          /env/bin/vadc-gwas-tools GetPheWebPlotJson \
          --in_tsv {{inputs.artifacts.pheweb_stats_tsv.path}} \
          --out_json /mnt/vol/{{inputs.parameters.out_prefix}}.pheweb.manhattan.json \
          --out_plot_type manhattan

          /env/bin/vadc-gwas-tools GetPheWebPlotJson \
          --in_tsv {{inputs.artifacts.pheweb_stats_tsv.path}} \
          --out_json /mnt/vol/{{inputs.parameters.out_prefix}}.pheweb.qq.json \
          --out_plot_type qq
        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
        resources:
          requests:
            cpu: 250m
            memory: 1Gi
          limits:
            cpu: 500m
            memory: 4Gi
      outputs:
        artifacts:
          - name: pheweb_manhattan_json
            path: "/mnt/vol/{{inputs.parameters.out_prefix}}.pheweb.manhattan.json"
            archive:
              none: {}
            s3:
              endpoint: s3.amazonaws.com
              bucket: "{{inputs.parameters.downloadable_bucket}}"
              key: "{{workflow.name}}/{{workflow.name}}.{{inputs.parameters.out_prefix}}.pheweb.manhattan.json"
            #   accessKeySecret:
            #     name: argo-s3-creds
            #     key: AccessKeyId
            #   secretKeySecret:
            #     name: argo-s3-creds
            #     key: SecretAccessKey
          - name: pheweb_qq_json
            path: "/mnt/vol/{{inputs.parameters.out_prefix}}.pheweb.qq.json"
            archive:
              none: {}
            s3:
              endpoint: s3.amazonaws.com
              bucket: "{{inputs.parameters.downloadable_bucket}}"
              key: "{{workflow.name}}/{{workflow.name}}.{{inputs.parameters.out_prefix}}.pheweb.qq.json"
            #   accessKeySecret:
            #     name: argo-s3-creds
            #     key: AccessKeyId
            #   secretKeySecret:
            #     name: argo-s3-creds
            #     key: SecretAccessKey

    - name: curate-gwas-hits
      inputs:
        parameters:
          - name: pvalue_cutoff
          - name: top_n_hits
          - name: out_prefix
        artifacts:
          - name: annotated_stats_dir
            mode: 511
            path: "/mnt/vol/{{workflow.name}}/annotated_summary_csvs"
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      script:
        image: 707767160287.dkr.ecr.us-east-1.amazonaws.com/gen3/vadc-gwas-tools:feat_database_version
        imagePullPolicy: IfNotPresent
        command: [/bin/bash]
        source: |
          /env/bin/vadc-gwas-tools CurateGwasHits \
          --pvalue_cutoff {{inputs.parameters.pvalue_cutoff}} \
          --top_n_hits {{inputs.parameters.top_n_hits}} \
          --summary_stats_dir {{inputs.artifacts.annotated_stats_dir.path}} \
          --out_prefix /mnt/vol/{{inputs.parameters.out_prefix}}

        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
        resources:
          requests:
            cpu: 250m
            memory: 1Gi
          limits:
            cpu: 500m
            memory: 4Gi
      outputs:
        artifacts:
          - name: top_hits_csv
            path: /mnt/vol/{{inputs.parameters.out_prefix}}.top_{{inputs.parameters.top_n_hits}}_hits.csv.gz
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/{{pod.name}}/{{inputs.parameters.out_prefix}}.top_{{inputs.parameters.top_n_hits}}_hits.csv.gz"
          - name: below_cutoff_csv
            path: /mnt/vol/{{inputs.parameters.out_prefix}}.below_cutoff_hits.csv.gz
            archive:
              none: {}
            s3:
              key: "{{workflow.name}}/{{pod.name}}/{{inputs.parameters.out_prefix}}.below_cutoff_hits.csv.gz"

    - name: archive-outputs
      inputs:
        parameters:
          - name: out_prefix
          - name: downloadable_bucket
        artifacts:
          - name: manhattan_plot
            path: "/mnt/vol/{{workflow.name}}/{{inputs.parameters.out_prefix}}_manhattan.png"
          - name: qq_plot 
            path: "/mnt/vol/{{workflow.name}}/{{inputs.parameters.out_prefix}}_qq.png"
          - name: annotated_csvs
            mode: 511
            path: "/mnt/vol/{{workflow.name}}/annotated_summary_csvs"
          - name: gwas_metadata
            path: "/mnt/vol/{{workflow.name}}/{{workflow.name}}.gwas_metadata.yaml"
          - name: attrition_csvs
            mode: 511
            path: "/mnt/vol/{{workflow.name}}/attrition_tables"
          - name: top_hits_csv
            path: "/mnt/vol/{{workflow.name}}/{{inputs.parameters.out_prefix}}.top_hits.csv.gz"
          - name: below_cutoff_csv
            path: "/mnt/vol/{{workflow.name}}/{{inputs.parameters.out_prefix}}.below_cutoff_hits.csv.gz"
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      script:
        image: quay.io/cdis/alpine-jq:latest
        imagePullPolicy: IfNotPresent
        command: [/bin/sh]
        source: |
          cd /mnt/vol/{{workflow.name}}/
          gzip -d {{inputs.parameters.out_prefix}}.below_cutoff_hits.csv.gz
          zip {{inputs.parameters.out_prefix}}.below_cutoff_hits.csv.zip {{inputs.parameters.out_prefix}}.below_cutoff_hits.csv
          rm {{inputs.parameters.out_prefix}}.below_cutoff_hits.csv

          gzip -d {{inputs.parameters.out_prefix}}.top_hits.csv.gz
          zip {{inputs.parameters.out_prefix}}.top_hits.csv.zip {{inputs.parameters.out_prefix}}.top_hits.csv
          rm {{inputs.parameters.out_prefix}}.top_hits.csv

          cd /mnt/vol/{{workflow.name}}/annotated_summary_csvs/
          for file in ./*; do gzip -d "$file"; done
          for file in ./*; do zip "$file.zip" "$file" & done; wait
          rm -r *.csv

          cd /mnt/vol/
          zip -r {{workflow.name}}.zip {{workflow.name}}/
        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
        resources:
          requests:
            cpu: 250m
            memory: 500Mi
          limits:
            cpu: 500m
            memory: 1Gi
      outputs:
        artifacts:
          - name: gwas_archive
            path: "/mnt/vol/{{workflow.name}}.zip"
            archive:
              none: {}
            s3:
              endpoint: s3.amazonaws.com
              bucket: "{{inputs.parameters.downloadable_bucket}}" 
              key: "{{workflow.name}}/{{workflow.name}}.zip"
            #   accessKeySecret:
            #     name: argo-s3-creds
            #     key: AccessKeyId
            #   secretKeySecret:
            #     name: argo-s3-creds
            #     key: SecretAccessKey

    - name: create-indexd-record
      inputs:
        parameters:
          - name: arborist_resource
          - name: internal_api_env
          - name: out_prefix
          - name: downloadable_bucket
        artifacts:
          - name: gwas_archive
            path: "/mnt/vol/{{workflow.name}}.zip"
          - name: manhattan_plot
            path: "/mnt/vol/{{workflow.name}}.{{inputs.parameters.out_prefix}}_manhattan.png"
          - name: attrition_json
            path: "/mnt/vol/{{workflow.name}}.attrition.json"
          - name: pheweb_manhattan_json
            path: "/mnt/vol/pheweb.manhattan.json"
          - name: pheweb_qq_json
            path: "/mnt/vol/pheweb.qq.json"
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
      container:
        image: 707767160287.dkr.ecr.us-east-1.amazonaws.com/gen3/vadc-gwas-tools:feat_database_version
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash", "-c"]
        args: ["/env/bin/vadc-gwas-tools CreateIndexdRecord \
                --gwas_archive {{inputs.artifacts.gwas_archive.path}} \
                --s3_uri s3://{{inputs.parameters.downloadable_bucket}}/{{workflow.name}}/{{workflow.name}}.zip \
                --arborist_resource {{inputs.parameters.arborist_resource}} \
                -o /mnt/vol/did_archive.json && \
                /env/bin/vadc-gwas-tools CreateIndexdRecord \
                --gwas_archive {{inputs.artifacts.manhattan_plot.path}} \
                --s3_uri s3://{{inputs.parameters.downloadable_bucket}}/{{workflow.name}}/{{workflow.name}}.{{inputs.parameters.out_prefix}}_manhattan.png \
                --arborist_resource {{inputs.parameters.arborist_resource}} \
                -o /mnt/vol/did_plot.json && \
                /env/bin/vadc-gwas-tools CreateIndexdRecord \
                --gwas_archive {{inputs.artifacts.attrition_json.path}} \
                --s3_uri s3://{{inputs.parameters.downloadable_bucket}}/{{workflow.name}}/{{workflow.name}}.attrition.json \
                --arborist_resource {{inputs.parameters.arborist_resource}} \
                -o /mnt/vol/did_attrition.json && \
                /env/bin/vadc-gwas-tools CreateIndexdRecord \
                --gwas_archive {{inputs.artifacts.pheweb_manhattan_json.path}} \
                --s3_uri s3://{{inputs.parameters.downloadable_bucket}}/{{workflow.name}}/{{workflow.name}}.{{inputs.parameters.out_prefix}}.pheweb.manhattan.json \
                --arborist_resource {{inputs.parameters.arborist_resource}} \
                -o /mnt/vol/did_pheweb_manhattan_json.json && \
                /env/bin/vadc-gwas-tools CreateIndexdRecord \
                --gwas_archive {{inputs.artifacts.pheweb_qq_json.path}} \
                --s3_uri s3://{{inputs.parameters.downloadable_bucket}}/{{workflow.name}}/{{workflow.name}}.{{inputs.parameters.out_prefix}}.pheweb.qq.json \
                --arborist_resource {{inputs.parameters.arborist_resource}} \
                -o /mnt/vol/did_pheweb_qq_json.json"]
        env:
          - name: GEN3_ENVIRONMENT
            value: "{{inputs.parameters.internal_api_env}}"
          - name: INDEXDUSER
            valueFrom:
              secretKeyRef:
                name: indexd-creds
                key: user
          - name: INDEXDPASS
            valueFrom:
              secretKeyRef:
                name: indexd-creds
                key: password
        volumeMounts:
          - name: workdir
            mountPath: /mnt/vol
        resources:
          requests:
            cpu: 50m
            memory: 50Mi
          limits:
            cpu: 100m
            memory: 100Mi
      outputs:
        parameters:
          - name: gwas_archive_index
            valueFrom:
              path: /mnt/vol/did_archive.json
            globalName: gwas_archive_index
          - name: manhattan_plot_index
            valueFrom:
              path: /mnt/vol/did_plot.json
            globalName: manhattan_plot_index
          - name: attrition_json_index
            valueFrom:
              path: /mnt/vol/did_attrition.json
            globalName: attrition_json_index
          - name: pheweb_manhattan_json_index
            valueFrom:
              path: /mnt/vol/did_pheweb_manhattan_json.json
            globalName: pheweb_manhattan_json_index
          - name: pheweb_qq_json_index
            valueFrom:
              path: /mnt/vol/did_pheweb_qq_json.json
            globalName: pheweb_qq_json_index
